<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chengguang (Claude) Xu</title>

  <meta name="author" content="Chengguang Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
  <td style="padding:0px">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:2.5%;width:63%;vertical-align:middle">
        <p class="name" style="text-align: center;">
          Chengguang (Claude) Xu
        </p>
        <p>Hi, I'm a final-year Ph.D. candidate at <a href="https://www.northeastern.edu/">
          Northeastern University</a> in Boston, where I work closely with
          <a href="https://www.khoury.northeastern.edu/people/lawson-wong/">Professor Lawson L.S. Wong</a> and
          <a href="https://www.khoury.northeastern.edu/people/chris-amato/">Professor Christopher Amato</a>.
          My research interests span machine learning, computer vision,
          and natural language processing, particularly at their intersection with robotics. My long-term goal is to
          equip robots with human-level multi-modality intelligence, including advanced visual perception for better
          scene understanding and robust natural language grounding for improved language-driven robot control.
        </p>
        <p>
          Before joining Northeastern, I earned my Master's and Bachelor's degrees from Nankai University in China.
        </p>
        <p style="text-align:center">
          <a href="data/Resume_Chengguang_Xu.pdf">Resume</a> &nbsp;/
          <a href="https://scholar.google.com/citations?user=F_yEpGIAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
          <a href="https://www.linkedin.com/in/chengguang-xu-2322171b7/">Linkedin</a>
        </p>
      </td>
      <td style="padding:2.5%;width:40%;max-width:40%">
        <a href="images/chengguang.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/chengguang.png" class="hoverZoomLink"></a>
      </td>
    </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Contact</h2>
        <p>
          If you are interested in my research or potential collaboration, you can reach me at: xu [dot] cheng [at] northeastern [dot] edu.
        </p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Research</h2>
        <p>
          Recently, I have been working on the cross-modality learning and grounding problems in
          <a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Vision-and-Language_
                    Navigation_Interpreting_CVPR_2018_paper.html">vision-and-language navigation (VLN)</a>.
          I am also exploring the <a href="https://www.sciencedirect.com/science/article/pii/S000437029800023X">
          sequential decision-making problem under partial observability</a> in different object goal navigation tasks.
          Please feel free to contact me if you are interested! xu dot cheng at northeastern dot edu
        </p>
      </td>
    </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="cmn_stop()" onmouseover="cmn_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cmn_image'><video  width=100% muted autoplay loop>
            <source src="images/cmn.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video></div>
          <img src='images/cmn_task.png' width=100%>
        </div>
        <script type="text/javascript">
          function cmn_start() {
            document.getElementById('cmn_image').style.opacity = "1";
          }

          function cmn_stop() {
            document.getElementById('cmn_image').style.opacity = "0";
          }
          cmn_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.khoury.northeastern.edu/home/lsw/papers/icra2024-cmn.pdf">
          <span class="papertitle">Robot Navigation in Unseen Environments using Coarse Maps</span>
        </a>
        <br>
        <strong>Chengguang Xu</strong>,
        Christopher Amato,
        Lawson L.S. Wong*
        <br>
        <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024
        <br>
        <!--              <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>-->
        <!--              /-->
        <a href="https://www.youtube.com/watch?v=iQhFEemFYT8">video</a>/
        <a href="data/ICRA24_Poster.pdf">poster</a>
        <p></p>
        <p>
          Propose a vision-based navigation system that can localize an embodied agent on top-down 2-D coarse maps (e.g., hand-drawn maps) from photo-realistic panoramic RGB images.
        </p>
      </td>
    </tr>
    <tr onmouseout="vlnce_stop()" onmouseover="vlnce_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='vlnce_image'><video  width=100% muted autoplay loop>
            <source src="images/vlnce.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video></div>
          <img src='images/vlnce_task.png' width=100%>
        </div>
        <script type="text/javascript">
          function vlnce_start() {
            document.getElementById('vlnce_image').style.opacity = "1";
          }

          function vlnce_stop() {
            document.getElementById('vlnce_image').style.opacity = "0";
          }
          vlnce_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=vyH9nAgmul">
          <span class="papertitle">Vision-and-Language Navigation in Real World using Foundation Models</span>
        </a>
        <br>
        <strong>Chengguang Xu*</strong>,
        Hieu Trung Nguyen,
        Christopher Amato,
        Lawson L.S. Wong
        <br>
        <em>In the Workshop of Foundation Models for Decision Making @ NeurIPS</em>, 2023
        <br>
        <a href="https://www.youtube.com/watch?v=wUyLKX4pF60">video</a>
        /
        <a href="data/NeurIPs23_FMDM_poster.pdf">poster</a>
        <p></p>
        <p>
          Propose a navigation framework that achieves zero-shot vision-and-language navigation in real world scenarios using a Large Language Model (LLM) and a Large Visual-Language Model(VLM).
        </p>
      </td>
    </tr>

    <tr onmouseout="genav_stop()" onmouseover="genav_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='genav_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/genav.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video></div>
          <img src='images/genav_task.png' width="160">
        </div>
        <script type="text/javascript">
          function genav_start() {
            document.getElementById('genav_image').style.opacity = "1";
          }

          function genav_stop() {
            document.getElementById('genav_image').style.opacity = "0";
          }
          genav_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2106.03665">
          <span class="papertitle">Hierarchical Robot Navigation in Novel Environments using Rough 2-D Maps</span>
        </a>
        <br>
        <strong>Chengguang Xu*</strong>,
        Christopher Amato,
        Lawson L.S. Wong
        <br>
        <em>Conference on Robot Learning (CoRL)</em>, 2020
        <br>
        <!--        <a href="https://reconfusion.github.io/">project page</a>-->
        <!--        /-->
        <p></p>
        <p>
          Propose a hierarchical navigation framework to facilitate high-level long-horizon planning and low-level goal-conditioned policy learning. To bridge the high-level map, we utilized a conditional generative model to generate RGB image goals from binary occupancy grids.
        </p>
      </td>
    </tr>


    <tr onmouseout="supfl_stop()" onmouseover="supfl_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='supfl_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/supfl_video.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video></div>
          <img src='images/supfl_task.png' width="160">
        </div>
        <script type="text/javascript">
          function supfl_start() {
            document.getElementById('supfl_image').style.opacity = "1";
          }

          function supfl_stop() {
            document.getElementById('supfl_image').style.opacity = "0";
          }
          supfl_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.neurips.cc/paper/2019/hash/3a066bda8c96b9478bb0512f0a43028c-Abstract.html">
          <span class="papertitle">Deep Supervised Summarization: Algorithm and Application to Learning Instructions
    </span>
        </a>
        <br>
        <strong>Chengguang Xu*</strong>,
        Ehsan Elhamifar
        <br>
        <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2019
        <br>
        <!--a href="https://proceedings.neurips.cc/paper/2019/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf">paper</a-->
        <p></p>
        <p>
          Propose a triple loss to learn sequential neural representations of video clips for supervised video summarization.
        </p>
      </td>


    <tr onmouseout="codrive_stop()" onmouseover="codrive_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='codrive_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/codrive_video.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video></div>
          <img src='images/codrive_task.png' width="160">
        </div>
        <script type="text/javascript">
          function codrive_start() {
            document.getElementById('supfl_image').style.opacity = "1";
          }

          function codrive_stop() {
            document.getElementById('supfl_image').style.opacity = "0";
          }
          codrive_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/8661605">
          <span class="papertitle">Design and Performance Evaluation of a Simple Semi-Physical Human-Vehicle Collaborative Driving Simulation System
          </span>
        </a>
        <br>
        Wenyu Li*,
        Feng Duan,
        <strong>Chengguang Xu</strong>
        <br>
        <em>IEEE Access</em>, 2019
        <br>
        <!--a href="https://ieeexplore.ieee.org/abstract/document/8661605">paper</a-->
        <p></p>
        <p>
          We design a co-pilot driving system that uses vision controller as a vehicle assistant.
        </p>
      </td>

    <tr onmouseout="brain_vis_drive_stop()" onmouseover="brain_vis_drive_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='brain_vis_drive_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/brain_vis_drive_video.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video></div>
          <img src='images/brain_vis_drive_task.png' width="160">
        </div>
        <script type="text/javascript">
          function brain_vis_drive_start() {
            document.getElementById('supfl_image').style.opacity = "1";
          }

          function brain_vis_drive_stop() {
            document.getElementById('supfl_image').style.opacity = "0";
          }
          brain_vis_drive_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/8082510?casa_token=YbfNAgXszOoAAAAA:m6f1sNJ05QFWMy4CSCf4GEpn63BlHV5h5AOjn7h8SwCrih7KZ0HJdwJkOXZpXGIMrbCTHlGGt70"><span class="papertitle">A Human-Vehicle Collaborative Simulated Driving System Based on Hybrid Brainâ€“Computer Interfaces and Computer Vision          </span>
        </a>
        <br>
        Wenyu Li,
        Feng Duan*,
        Shili Sheng,
        <strong>Chengguang Xu</strong>,
        Rensong Liu,
        Zhiwen Zhang,
        Xue Jiang
        <br>
        <em>IEEE Transactions on Cognitive and Developmental Systems</em>, 2018
        <br>
        <!--a href="https://ieeexplore.ieee.org/document/8082510">paper</a-->
        <p></p>
        <p>
          We design a co-pilot driving system that uses vision controller as a vehicle assistant.
        </p>
      </td>
    </tr>





    </tbody></table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
    <tr>
      <td>
        <!--                <h2>Miscellanea</h2>-->
      </td>
    </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>

    <tr>
      <!--              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
      <td width="75%" valign="center">

      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <!--                <img src="images/cs188.jpg" alt="cs188">-->
      </td>
      <td width="75%" valign="center">

      </td>
    </tr>


    <tr>
      <td align="center" style="padding:20px;width:25%;vertical-align:middle">
        <!--                <h2>Basically <br> Blog Posts</h2>-->
      </td>
      <td width="75%" valign="middle">
      </td>
    </tr>


    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
        </p>
      </td>
    </tr>
    </tbody></table>
  </td>
</tr>
</table>
</body>
</html>
